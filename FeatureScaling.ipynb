{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a611bc4-91be-4ba2-b101-209beb25c919",
   "metadata": {},
   "source": [
    "# Feature Scaling:\n",
    "Feature Scaling means transforming the values of numerical features to have similar scale or range ensuring all features contribute equally to the model.<br>\n",
    "This helps machine learning models perform better by preventing some features from dominating others just because of their larger values.<br><br>\n",
    "**Why It's Important**<br>\n",
    "Imagine you have a dataset with two features:<br>\n",
    "\n",
    "Salary, ranging from 20,000  to 200,000.<br>\n",
    "\n",
    "Age, ranging from 18 to 70.<br>\n",
    "\n",
    "A model like a Support Vector Machine (SVM) or K-Nearest Neighbors (KNN) uses the distance between data points to make predictions. Without scaling, the \"Salary\" feature would dominate the distance calculation simply because its values are much larger than the \"Age\" values. This could lead to a biased model that incorrectly prioritizes one feature over another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8c28f-e784-4edb-8064-73cc8e042006",
   "metadata": {},
   "source": [
    "# Common Methods of Feature Scaling\n",
    "\n",
    "# 1.Normalization (Min-Max Scaling)\n",
    "Transforms values to a range between 0 and 1<br>\n",
    "$$\n",
    "X_{normalized} = \\left( \\frac{X - X_{min}}{X_{max} - X_{min}} \\right)\n",
    "$$\n",
    "where X is feature value, Xmin is the minimum feature value in the dataset, and Xmax is the maximum feature value.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "936cb4d7-cae5-4a9b-b7b0-8b38707faa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Income\n",
      "0   25   50000\n",
      "1   45  100000\n",
      "2   35   75000\n",
      "3   50  120000\n",
      "4   23   35000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Age\": [25, 45, 35, 50, 23],\n",
    "    \"Income\": [50000, 100000, 75000, 120000, 35000]\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15716e35-dcc0-4194-8806-1c8445fe3e32",
   "metadata": {},
   "source": [
    "Income has values in tens of thousands, whereas age is in tens . Models might give more importance to Income unless scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6ac4a16-a48b-40e1-9618-b05b2241fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07407407 0.17647059]\n",
      " [0.81481481 0.76470588]\n",
      " [0.44444444 0.47058824]\n",
      " [1.         1.        ]\n",
      " [0.         0.        ]] \n",
      "\n",
      "        Age    Income\n",
      "0  0.074074  0.176471\n",
      "1  0.814815  0.764706\n",
      "2  0.444444  0.470588\n",
      "3  1.000000  1.000000\n",
      "4  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() #create instance of MinMaxScaler\n",
    "\n",
    "scaled_data = scaler.fit_transform(df) # gives a two numpy array\n",
    "print(scaled_data,\"\\n\")\n",
    "df['Age'],df['Income']=scaled_data[:,0],scaled_data[:,1] #[:,0] --> select all rows from column 0(Age) and [:,1]-->select all rows from column 1(Income)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f16d99-8e01-4ff4-8daf-fbb055c80a58",
   "metadata": {},
   "source": [
    "**Drawback**<br>\n",
    "i)Sensitive to outilers:<br>\n",
    ">Example:<br>\n",
    "Original Age data → [25, 35, 40, 30, 150]<br>\n",
    "\n",
    "* Here, 150 is an outlier.<br>\n",
    "\n",
    "After scaling, most values might lie between 0 and 0.2, while the outlier reaches 1 → the pattern of the majority is lost.\n",
    "<br>\n",
    "<br>\n",
    "**common uses**<br>\n",
    "Its mostly used in K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and K-Means Clustering,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8026174-a6e5-4550-870f-54e4daa31a9b",
   "metadata": {},
   "source": [
    "# 2.Vector Normalization\n",
    "A vector is just a list of numbers that represents features of a data point.<br>\n",
    "For example:<br>\n",
    "\n",
    "    \"Age\": [25, 45, 35, 50, 23],\n",
    "    \"Income\": [50000, 100000, 75000, 120000, 35000]\n",
    "\n",
    "V=[25,50000] is a vector.<br>\n",
    "This vector can be thought of as a point in 2D space (x=25, y=50000), or as an arrow pointing from the origin (0,0) to the point (25,50000).<br>\n",
    "The direction of a vector is where it points in space — that is, the way the arrow is oriented.<br><br>\n",
    "For example:<br>\n",
    "[25, 50000] and [50, 100000] point in the same direction!\n",
    "They just differ in how long the arrow is.\n",
    "\n",
    "Now $$\n",
    "Length = \\sqrt{25^2 + 50000^2}\n",
    "$$\n",
    "\n",
    "**What is Vector Normalization?**<br>\n",
    "\n",
    "Vector normalization is a process where you adjust the values in a vector (or row of data) so that the vector’s length (or magnitude) becomes 1.<br><br>\n",
    "✔ It’s often used when the direction of the data matters more than its magnitude.<br>\n",
    "✔ This technique is common in algorithms like KNN, Cosine Similarity, Text Mining, and Neural Networks.<br>\n",
    "$$\n",
    "||v||= \\sqrt{x_{1}^2 + x_{2}^2+...+x_{n}^2}\n",
    "$$\n",
    "$$\n",
    "x_{i-normalized} = \\frac{x_i}{||v||}\n",
    "$$\n",
    "This way, the normalized vector has a length of 1, but keeps its direction same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bcf50b8f-9908-4b98-bea6-d16a37118af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Income\n",
      "0   25   50000\n",
      "1   45  100000\n",
      "2   35   75000\n",
      "3   50  120000\n",
      "4   23   35000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Age\": [25, 45, 35, 50, 23],\n",
    "    \"Income\": [50000, 100000, 75000, 120000, 35000]\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "18c96785-666f-4d75-ac3e-a6212edfeeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.99999938e-04 9.99999875e-01]\n",
      " [4.49999954e-04 9.99999899e-01]\n",
      " [4.66666616e-04 9.99999891e-01]\n",
      " [4.16666630e-04 9.99999913e-01]\n",
      " [6.57142715e-04 9.99999784e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "df_normalized=normalizer.fit_transform(df)\n",
    "print(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22d7bf-cc81-4d05-a435-e7008363197a",
   "metadata": {},
   "source": [
    "NOTE: Normalization doesn’t change proportions. It simply scales the vector so its length is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b87d72-45a4-48b6-8260-e5da1c4eec77",
   "metadata": {},
   "source": [
    "# 3.Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7080f97-f1c5-461f-9ab5-df482dd4934f",
   "metadata": {},
   "source": [
    "Mean normalization is a feature scaling technique where you shift and scale the data so that the features have a mean of 0.<br>\n",
    "$$\n",
    "X_{normalized} = \\left( \\frac{X - mean(X)}{X_{max} - X_{min}} \\right)\n",
    "$$\n",
    "Why is it used?\n",
    "\n",
    "1. Centering the data helps many algorithms (like linear regression, gradient descent, neural networks) perform better.\n",
    "\n",
    "2. It prevents large-valued features from dominating models.\n",
    "\n",
    "3. It maintains relationships while adjusting the scale.\n",
    "\n",
    "A negative value means the original value was below the mean. A positive value means the original value was above the mean\n",
    "\n",
    "The magnitude shows how far the value is from the average relative to the spread.\n",
    "\n",
    "It is sensitive to ouliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9ef69-6349-48a8-b2f6-622a2d0af19e",
   "metadata": {},
   "source": [
    "# 4. Absolute Maximum Scaling\n",
    "Absolute Maximum Scaling rescales each feature by dividing all values by the maximum absolute value of that feature. This ensures the feature values fall within the range of -1 to 1.<br>\n",
    "\n",
    "Sensitive to outliers, making it less suitable for noisy datasets.<br>\n",
    "\n",
    "$$\n",
    "X_{scaled}=\\frac{X_{i}}{|X_{max}|}\n",
    "$$\n",
    "\n",
    "It is also sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ede2ec-e4a3-4430-a548-55ad0dad89a5",
   "metadata": {},
   "source": [
    "# 5.Robust Scaling\n",
    "Robust Scaling uses the median and interquartile range (IQR) making the transformation robust to outliers and skewed distributions.<br>\n",
    "\n",
    "The median and IQR are robust statistics → they don’t change much when extreme values are present.\n",
    "So, this scaler is called RobustScaler in scikit-learn<br>\n",
    "$$\n",
    "X_{Scaled} = \\left( \\frac{X - median(X)}{IQR} \\right)\n",
    "$$\n",
    "Where:\n",
    "\n",
    "Median(X) = the 50th percentile (middle value)\n",
    "\n",
    "IQR = Q3 - Q1 (75th percentile – 25th percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f64222fe-13b4-4fea-9356-d138823c8ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Income\n",
      "0    50000\n",
      "1    60000\n",
      "2    55000\n",
      "3    58000\n",
      "4  1200000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "df = pd.DataFrame({\n",
    "    \"Income\": [50000, 60000, 55000, 58000, 1200000]  #last value is an outlier\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82ea9a2b-eb66-4f3d-a40f-52884c6da98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.6]\n",
      " [  0.4]\n",
      " [ -0.6]\n",
      " [  0. ]\n",
      " [228.4]]\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54836d2-bb9e-42f9-a0a5-34c8101665da",
   "metadata": {},
   "source": [
    "RobustScaler helps models learn better when outliers are present. It prevents extreme values from dominating the scaling process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5569f9-28e1-4cae-a3fc-2e72eb73560a",
   "metadata": {},
   "source": [
    "# 6.Standardization (Also known as Z-Score Normalization)\n",
    "\n",
    "Standardization is a feature scaling technique where you rescale the data so that it has:<br>\n",
    "* Mean = 0<br>\n",
    "* Standard deviation =1 <br>\n",
    "$$\n",
    "X_{Scaled} = \\left( \\frac{X_{i} -μ}{σ} \\right)\n",
    "$$\n",
    "\n",
    "✔ Handles outliers better than MinMax scaling (though not perfectly)<br>\n",
    "✔ Centers data and normalizes variance → improves model performance<br>\n",
    "✔ Effective for data approximately normally distributed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "828bc72d-7b81-4570-b2cf-2bc4547b3e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age   Income\n",
      "0   20    50000\n",
      "1   35    60000\n",
      "2   18    55000\n",
      "3   25    58000\n",
      "4   65  1200000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.DataFrame({\n",
    "    \"Age\":[20,35,18,25,65],\n",
    "    \"Income\": [50000, 60000, 55000, 58000, 1200000]  # outlier present\n",
    "})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "582da6dd-5178-4857-a677-e14c616208a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.73107693 -0.51254893]\n",
      " [ 0.13925275 -0.49070115]\n",
      " [-0.84712088 -0.50162504]\n",
      " [-0.44096703 -0.49507071]\n",
      " [ 1.8799121   1.99994582]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedb61d-81d7-465b-a19c-e5fc7ecd22c5",
   "metadata": {},
   "source": [
    "The values are now centered around 0, with most of them near 0 and the outlier slightly farther.This makes the data symmetrical and prevents the algorithm from being skewed by large numbers.<br>\n",
    "Centers the data → Mean becomes 0 and Normalizes variance → The differences between data points are scaled so that the spread of values is uniform.<br>\n",
    "All features contribute equally, even if they originally had different ranges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
